{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c131b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import requests\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# =============================================================================\n",
    "# SETUP AND CONFIGURATION\n",
    "# =============================================================================\n",
    "\n",
    "# Ollama typically runs on localhost:11434\n",
    "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
    "OLLAMA_API_URL = f\"{OLLAMA_BASE_URL}/v1\"\n",
    "\n",
    "# Configure OpenAI client to use Ollama\n",
    "client = openai.OpenAI(\n",
    "    base_url=OLLAMA_API_URL,\n",
    "    api_key=\"ollama\"  # Ollama doesn't require a real API key, but OpenAI client expects one\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cadcc6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# UTILITY FUNCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "def check_ollama_status():\n",
    "    \"\"\"Check if Ollama is running and accessible\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\")\n",
    "        if response.status_code == 200:\n",
    "            return True, \"Ollama is running successfully!\"\n",
    "        else:\n",
    "            return False, f\"Ollama responded with status code: {response.status_code}\"\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return False, f\"Cannot connect to Ollama: {e}\"\n",
    "\n",
    "def list_available_models():\n",
    "    \"\"\"List all available models in Ollama\"\"\"\n",
    "    try:\n",
    "        response = requests.get(f\"{OLLAMA_BASE_URL}/api/tags\")\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get('models', [])\n",
    "            return [model['name'] for model in models]\n",
    "        else:\n",
    "            return []\n",
    "    except requests.exceptions.RequestException:\n",
    "        return []\n",
    "\n",
    "def pull_model(model_name: str):\n",
    "    \"\"\"Pull a model from Ollama registry\"\"\"\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            f\"{OLLAMA_BASE_URL}/api/pull\",\n",
    "            json={\"name\": model_name}\n",
    "        )\n",
    "        return response.status_code == 200\n",
    "    except requests.exceptions.RequestException:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e324cc47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking Ollama connection...\n",
      "Status: Ollama is running successfully!\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Checking Ollama connection...\")\n",
    "is_running, status_message = check_ollama_status()\n",
    "print(f\"Status: {status_message}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb2cc5f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Available models: ['llama3.2:latest']\n"
     ]
    }
   ],
   "source": [
    "if is_running:\n",
    "    available_models = list_available_models()\n",
    "    print(f\"\\nüì¶ Available models: {available_models}\")\n",
    "    \n",
    "    if not available_models:\n",
    "        print(\"\\n‚ö†Ô∏è  No models found! You may need to pull a model first.\")\n",
    "        print(\"Example: Run 'ollama pull llama2' in your terminal\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Please start Ollama first:\")\n",
    "    print(\"1. Install Ollama from https://ollama.ai\")\n",
    "    print(\"2. Run 'ollama serve' in terminal\")\n",
    "    print(\"3. Pull a model: 'ollama pull llama2' or 'ollama pull mistral'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f127fa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_chat_example(model_name: str = \"llama2\"):\n",
    "    \"\"\"Basic chat completion example\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Explain quantum computing in simple terms.\"}\n",
    "            ],\n",
    "            max_tokens=200,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "926e73c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ Using model: llama3.2:latest\n",
      "\n",
      "Response:\n",
      "Quantum computing is a new way of processing information that's different from classical computers like your smartphone or laptop.\n",
      "\n",
      "**Classical Computers**\n",
      "\n",
      "Classical computers use \"bits\" to store and process information. Bits can be either 0 or 1, and they're used to perform calculations by using simple arithmetic operations (like addition or subtraction).\n",
      "\n",
      "**Quantum Computers**\n",
      "\n",
      "Quantum computers, on the other hand, use \"qubits\" (quantum bits). Qubits are special because they can be both 0 AND 1 at the same time! This is known as a superposition.\n",
      "\n",
      "Imagine you have a coin that can either be heads or tails. A classical computer would say it's either one or the other, but a quantum computer would say it's both heads AND tails at the same time (until you measure it).\n",
      "\n",
      "**Entanglement**\n",
      "\n",
      "Another key feature of quantum computers is entanglement. When two qubits are connected (entangled), what happens to one qubit instantly\n"
     ]
    }
   ],
   "source": [
    "if available_models:\n",
    "    model_to_use = available_models[0]  # Use first available model\n",
    "    print(f\"\\nü§ñ Using model: {model_to_use}\")\n",
    "    result = simple_chat_example(model_to_use)\n",
    "    print(f\"\\nResponse:\\n{result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f38cc810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÑ Streaming example:\n",
      "**Factorial Function in Python**\n",
      "====================================\n",
      "\n",
      "Here's a simple recursive and iterative function to calculate the factorial of a given number.\n",
      "\n",
      "### Recursive Implementation\n",
      "\n",
      "```python\n",
      "def factorial_recursive(n):\n",
      "    \"\"\"\n",
      "    Calculates the factorial of a given number using recursion.\n",
      "    \n",
      "    Args:\n",
      "        n (int): The input number.\n",
      "    \n",
      "    Returns:\n",
      "        int: The factorial of n.\n",
      "    \n",
      "    Raises:\n",
      "        ValueError: If n is negative.\n",
      "    \"\"\"\n",
      "    if not isinstance(n, int):\n",
      "        raise TypeError(\"Input must be an integer.\")\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Factorial is not defined for negative numbers.\")\n",
      "    elif n == 0 or n == 1:\n",
      "        return 1\n",
      "    else:\n",
      "        return n * factorial_recursive(n-1)\n",
      "```\n",
      "\n",
      "### Iterative Implementation\n",
      "\n",
      "```python\n",
      "def factorial_iterative(n):\n",
      "    \"\"\"\n",
      "    Calculates the factorial of a given number using iteration.\n",
      "    \n",
      "    Args:\n",
      "        n (int): The input number.\n",
      "    \n",
      "    Returns:\n",
      "        int: The factorial of n.\n",
      "    \n",
      "    Raises:\n",
      "        ValueError: If n is negative.\n",
      "    \"\"\"\n",
      "    if not isinstance(n, int):\n",
      "        raise TypeError(\"Input must be an integer.\")\n",
      "    if n < 0:\n",
      "        raise ValueError(\"Factorial is not defined for negative numbers.\")\n",
      "    result = 1\n",
      "    for i in range(1, n+1):\n",
      "        result *= i\n",
      "    return result\n",
      "```\n",
      "\n",
      "### Example Usage\n",
      "\n",
      "```python"
     ]
    }
   ],
   "source": [
    "\n",
    "def streaming_chat_example(model_name: str, user_message: str):\n",
    "    \"\"\"Example of streaming chat completion\"\"\"\n",
    "    try:\n",
    "        stream = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": user_message}\n",
    "            ],\n",
    "            stream=True,\n",
    "            max_tokens=300,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        full_response = \"\"\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                content = chunk.choices[0].delta.content\n",
    "                print(content, end=\"\", flush=True)\n",
    "                full_response += content\n",
    "        \n",
    "        return full_response\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Example usage:\n",
    "if available_models:\n",
    "    print(\"\\nüîÑ Streaming example:\")\n",
    "    streaming_chat_example(available_models[0], \"Write a Python function to calculate factorial\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66eadb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OllamaConversation:\n",
    "    \"\"\"Handle multi-turn conversations with Ollama\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str, system_message: str = \"You are a helpful assistant.\"):\n",
    "        self.model_name = model_name\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "        self.client = client\n",
    "    \n",
    "    def add_user_message(self, content: str):\n",
    "        \"\"\"Add a user message to the conversation\"\"\"\n",
    "        self.messages.append({\"role\": \"user\", \"content\": content})\n",
    "    \n",
    "    def get_response(self, stream: bool = False, **kwargs):\n",
    "        \"\"\"Get response from the model\"\"\"\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=self.messages,\n",
    "                stream=stream,\n",
    "                **kwargs\n",
    "            )\n",
    "            \n",
    "            if stream:\n",
    "                full_response = \"\"\n",
    "                for chunk in response:\n",
    "                    if chunk.choices[0].delta.content is not None:\n",
    "                        content = chunk.choices[0].delta.content\n",
    "                        print(content, end=\"\", flush=True)\n",
    "                        full_response += content\n",
    "                self.messages.append({\"role\": \"assistant\", \"content\": full_response})\n",
    "                return full_response\n",
    "            else:\n",
    "                response_content = response.choices[0].message.content\n",
    "                self.messages.append({\"role\": \"assistant\", \"content\": response_content})\n",
    "                return response_content\n",
    "                \n",
    "        except Exception as e:\n",
    "            return f\"Error: {e}\"\n",
    "    \n",
    "    def reset_conversation(self, system_message: str = \"You are a helpful assistant.\"):\n",
    "        \"\"\"Reset the conversation\"\"\"\n",
    "        self.messages = [{\"role\": \"system\", \"content\": system_message}]\n",
    "    \n",
    "    def get_conversation_history(self):\n",
    "        \"\"\"Get the full conversation history\"\"\"\n",
    "        return self.messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f705062f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üí¨ Starting conversation with llama3.2:latest\n",
      "Assistant: **List Comprehensions in Python**\n",
      "\n",
      "A list comprehension is a compact way to create lists in Python. It consists of brackets containing an expression followed by a `for` clause, then zero or more `for` or `if` clauses. The result is a new list resulting from evaluating the expression in the context of the given control structures.\n",
      "\n",
      "**Basic Syntax**\n",
      "```python\n",
      "[expression for variable in iterable if condition]\n",
      "```\n",
      "**Example**\n",
      "```python\n",
      "numbers = [1, 2, 3, 4, 5]\n",
      "even_numbers = [num for num in numbers if num % 2 == 0]\n",
      "print(even_numbers)  # Output: [2, 4]\n",
      "```\n",
      "In this example, the\n",
      "\n",
      "Assistant: **Practical Example: Processing a List of Students**\n",
      "\n",
      "Suppose we have a list of students with their names and ages. We want to extract the names of all students who are older than 18.\n",
      "\n",
      "```python\n",
      "# Original list of students\n",
      "students = [\n",
      "    {\"name\": \"John\", \"age\": 19},\n",
      "    {\"name\": \"Alice\", \"age\": 17},\n",
      "    {\"name\": \"Bob\", \"age\": 20},\n",
      "    {\"name\": \"Charlie\", \"age\": 16}\n",
      "]\n",
      "\n",
      "# List comprehension to extract names of students older than 18\n",
      "older_students = [student[\"name\"] for student in students if student[\"age\"] > 18]\n",
      "print(older_students)  # Output: [\"\n",
      "\n",
      "üìù Conversation History:\n",
      "1. System: You are a Python programming tutor. Be concise and helpful....\n",
      "2. User: What is a list comprehension in Python?...\n",
      "3. Assistant: **List Comprehensions in Python**\n",
      "\n",
      "A list comprehension is a compact way to create lists in Python. ...\n",
      "4. User: Can you give me a practical example?...\n",
      "5. Assistant: **Practical Example: Processing a List of Students**\n",
      "\n",
      "Suppose we have a list of students with their ...\n"
     ]
    }
   ],
   "source": [
    "if available_models:\n",
    "    print(f\"\\nüí¨ Starting conversation with {available_models[0]}\")\n",
    "    \n",
    "    # Initialize conversation\n",
    "    conv = OllamaConversation(\n",
    "        model_name=available_models[0],\n",
    "        system_message=\"You are a Python programming tutor. Be concise and helpful.\"\n",
    "    )\n",
    "    \n",
    "    # Example conversation\n",
    "    conv.add_user_message(\"What is a list comprehension in Python?\")\n",
    "    response1 = conv.get_response(max_tokens=150)\n",
    "    print(f\"Assistant: {response1}\\n\")\n",
    "    \n",
    "    conv.add_user_message(\"Can you give me a practical example?\")\n",
    "    response2 = conv.get_response(max_tokens=150)\n",
    "    print(f\"Assistant: {response2}\\n\")\n",
    "    \n",
    "    # Print conversation history\n",
    "    print(\"üìù Conversation History:\")\n",
    "    for i, msg in enumerate(conv.get_conversation_history()):\n",
    "        print(f\"{i+1}. {msg['role'].title()}: {msg['content'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ac0c90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
