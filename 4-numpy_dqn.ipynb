{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "751b63d1-f59c-49e3-8220-c70f11be4e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ae00fc6-3c63-4af0-ab12-ee50e9a0cfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    \"\"\"Base class for neural network layers\"\"\"\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Forward pass - to be implemented by subclasses\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        \"\"\"Backward pass - to be implemented by subclasses\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Activation(Layer):\n",
    "    \"\"\"Base activation layer\"\"\"\n",
    "    def __init__(self, activation_func, activation_derivative):\n",
    "        super().__init__()\n",
    "        self.activation = activation_func\n",
    "        self.activation_derivative = activation_derivative\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Forward propagation for activation layer\"\"\"\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(input_data)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        \"\"\"Backward propagation for activation layer\"\"\"\n",
    "        return output_gradient * self.activation_derivative(self.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3817e70e-5c2c-4fb9-a1fb-de07ac95f381",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(Layer):\n",
    "    \"\"\"Fully connected (dense) layer\"\"\"\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Xavier/Glorot initialization\n",
    "        self.weights = np.random.randn(input_size, output_size) * np.sqrt(2.0 / input_size)\n",
    "        self.biases = np.zeros((1, output_size))\n",
    "        \n",
    "        # Store gradients for optimizer\n",
    "        self.weights_gradient = None\n",
    "        self.biases_gradient = None\n",
    "    \n",
    "    def forward(self, input_data):\n",
    "        \"\"\"Forward propagation for dense layer\"\"\"\n",
    "        self.input = input_data\n",
    "        return np.dot(input_data, self.weights) + self.biases\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate=None):\n",
    "        \"\"\"Backward propagation for dense layer\"\"\"\n",
    "        # Compute gradients\n",
    "        self.weights_gradient = np.dot(self.input.T, output_gradient) / self.input.shape[0]\n",
    "        self.biases_gradient = np.sum(output_gradient, axis=0, keepdims=True) / self.input.shape[0]\n",
    "        input_gradient = np.dot(output_gradient, self.weights.T)\n",
    "        \n",
    "        if learning_rate is not None:\n",
    "            self.weights -= learning_rate * self.weights_gradient\n",
    "            self.biases -= learning_rate * self.biases_gradient\n",
    "        \n",
    "        return input_gradient\n",
    "\n",
    "class ReLU(Activation):\n",
    "    \"\"\"ReLU activation layer\"\"\"\n",
    "    def __init__(self):\n",
    "        def relu(x):\n",
    "            return np.maximum(0, x)\n",
    "        \n",
    "        def relu_derivative(x):\n",
    "            return (x > 0).astype(float)\n",
    "        \n",
    "        super().__init__(relu, relu_derivative)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bee8418d-1b91-42f0-909e-aca0ffc11999",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    \"\"\"Base optimizer class\"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"Update layer parameters - to be implemented by subclasses\"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "class Adam(Optimizer):\n",
    "    \"\"\"Adam optimizer\"\"\"\n",
    "    def __init__(self, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        super().__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.t = 0  # time step\n",
    "        \n",
    "        # Store moment estimates for each layer\n",
    "        self.m_weights = {}  # First moment (momentum)\n",
    "        self.v_weights = {}  # Second moment (RMSprop)\n",
    "        self.m_biases = {}\n",
    "        self.v_biases = {}\n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"Update parameters using Adam optimizer\"\"\"\n",
    "        if not isinstance(layer, Dense) or layer.weights_gradient is None:\n",
    "            return\n",
    "        # Get layer id (use object id as unique identifier)\n",
    "        layer_id = id(layer)\n",
    "        \n",
    "        # Initialize moment estimates if first time\n",
    "        if layer_id not in self.m_weights:\n",
    "            self.m_weights[layer_id] = np.zeros_like(layer.weights)\n",
    "            self.v_weights[layer_id] = np.zeros_like(layer.weights)\n",
    "            self.m_biases[layer_id] = np.zeros_like(layer.biases)\n",
    "            self.v_biases[layer_id] = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # Increment time step\n",
    "        self.t += 1\n",
    "        \n",
    "        # Update biased first moment estimate for weights\n",
    "        self.m_weights[layer_id] = (self.beta1 * self.m_weights[layer_id] + \n",
    "                                   (1 - self.beta1) * layer.weights_gradient)\n",
    "        \n",
    "        # Update biased second moment estimate for weights\n",
    "        self.v_weights[layer_id] = (self.beta2 * self.v_weights[layer_id] + \n",
    "                                   (1 - self.beta2) * (layer.weights_gradient ** 2))\n",
    "        \n",
    "        # Update biased first moment estimate for biases\n",
    "        self.m_biases[layer_id] = (self.beta1 * self.m_biases[layer_id] + \n",
    "                                  (1 - self.beta1) * layer.biases_gradient)\n",
    "        \n",
    "        # Update biased second moment estimate for biases\n",
    "        self.v_biases[layer_id] = (self.beta2 * self.v_biases[layer_id] + \n",
    "                                  (1 - self.beta2) * (layer.biases_gradient ** 2))\n",
    "        \n",
    "        # Compute bias-corrected first moment estimate\n",
    "        m_weights_corrected = self.m_weights[layer_id] / (1 - self.beta1 ** self.t)\n",
    "        m_biases_corrected = self.m_biases[layer_id] / (1 - self.beta1 ** self.t)\n",
    "        \n",
    "        # Compute bias-corrected second moment estimate\n",
    "        v_weights_corrected = self.v_weights[layer_id] / (1 - self.beta2 ** self.t)\n",
    "        v_biases_corrected = self.v_biases[layer_id] / (1 - self.beta2 ** self.t)\n",
    "        \n",
    "        # Update parameters\n",
    "        layer.weights -= (self.learning_rate * m_weights_corrected / \n",
    "                         (np.sqrt(v_weights_corrected) + self.epsilon))\n",
    "        layer.biases -= (self.learning_rate * m_biases_corrected / \n",
    "                        (np.sqrt(v_biases_corrected) + self.epsilon))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa20184b-10c4-4835-adee-8079ecf4b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, in_states, h1_nodes, out_actions):\n",
    "        self.l1 = Dense(in_states, h1_nodes)\n",
    "        self.ac1 = ReLU()\n",
    "        self.l2 = Dense(h1_nodes, out_actions)\n",
    "        self.ac2 = ReLU()\n",
    "\n",
    "        self.layers = [self.l1, self.ac1, self.l2, self.ac2]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return \\\n",
    "            self.ac2.forward(\n",
    "                self.l2.forward(\n",
    "                    self.ac1.forward(\n",
    "                        self.l1.forward(x)\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Get the current state of the network including weights, biases, and optimizer state\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary containing all network parameters and optimizer state\n",
    "        \"\"\"\n",
    "        state = {\n",
    "            'layers': [],\n",
    "            'optimizer_state': None,\n",
    "            'optimizer_type': self.optimizer.__class__.__name__,\n",
    "            'optimizer_params': {}\n",
    "        }\n",
    "        \n",
    "        # Save layer parameters\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if isinstance(layer, Dense):\n",
    "                layer_state = {\n",
    "                    'type': 'Dense',\n",
    "                    'input_size': layer.input_size,\n",
    "                    'output_size': layer.output_size,\n",
    "                    'weights': layer.weights.copy(),\n",
    "                    'biases': layer.biases.copy()\n",
    "                }\n",
    "                state['layers'].append(layer_state)\n",
    "            elif isinstance(layer, Activation):\n",
    "                layer_state = {\n",
    "                    'type': layer.__class__.__name__\n",
    "                }\n",
    "                state['layers'].append(layer_state)\n",
    "        \n",
    "        # Save optimizer parameters\n",
    "        if hasattr(self.optimizer, 'learning_rate'):\n",
    "            state['optimizer_params']['learning_rate'] = self.optimizer.learning_rate\n",
    "        \n",
    "        if isinstance(self.optimizer, Adam):\n",
    "            state['optimizer_params'].update({\n",
    "                'beta1': self.optimizer.beta1,\n",
    "                'beta2': self.optimizer.beta2,\n",
    "                'epsilon': self.optimizer.epsilon\n",
    "            })\n",
    "            \n",
    "            # Save Adam optimizer state (moment estimates)\n",
    "            optimizer_state = {\n",
    "                't': self.optimizer.t,\n",
    "                'm_weights': {str(k): v.copy() for k, v in self.optimizer.m_weights.items()},\n",
    "                'v_weights': {str(k): v.copy() for k, v in self.optimizer.v_weights.items()},\n",
    "                'm_biases': {str(k): v.copy() for k, v in self.optimizer.m_biases.items()},\n",
    "                'v_biases': {str(k): v.copy() for k, v in self.optimizer.v_biases.items()}\n",
    "            }\n",
    "            state['optimizer_state'] = optimizer_state\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    def load_state(self, state):\n",
    "        \"\"\"\n",
    "        Load network state from a previously saved state\n",
    "        \n",
    "        Args:\n",
    "            state (dict): State dictionary from get_state()\n",
    "        \"\"\"\n",
    "        # Clear existing layers\n",
    "        self.layers = []\n",
    "        \n",
    "        # Recreate layers from state\n",
    "        for layer_state in state['layers']:\n",
    "            if layer_state['type'] == 'Dense':\n",
    "                layer = Dense(layer_state['input_size'], layer_state['output_size'])\n",
    "                layer.weights = layer_state['weights'].copy()\n",
    "                layer.biases = layer_state['biases'].copy()\n",
    "                self.layers.append(layer)\n",
    "            elif layer_state['type'] == 'Sigmoid':\n",
    "                self.layers.append(Sigmoid())\n",
    "            elif layer_state['type'] == 'ReLU':\n",
    "                self.layers.append(ReLU())\n",
    "            elif layer_state['type'] == 'Tanh':\n",
    "                self.layers.append(Tanh())\n",
    "        \n",
    "        # Recreate optimizer\n",
    "        optimizer_type = state['optimizer_type']\n",
    "        optimizer_params = state['optimizer_params']\n",
    "        \n",
    "        if optimizer_type == 'SGD':\n",
    "            self.optimizer = SGD(learning_rate=optimizer_params.get('learning_rate', 0.01))\n",
    "        elif optimizer_type == 'Adam':\n",
    "            self.optimizer = Adam(\n",
    "                learning_rate=optimizer_params.get('learning_rate', 0.001),\n",
    "                beta1=optimizer_params.get('beta1', 0.9),\n",
    "                beta2=optimizer_params.get('beta2', 0.999),\n",
    "                epsilon=optimizer_params.get('epsilon', 1e-8)\n",
    "            )\n",
    "            \n",
    "            # Restore Adam optimizer state if available\n",
    "            if state['optimizer_state'] is not None:\n",
    "                opt_state = state['optimizer_state']\n",
    "                self.optimizer.t = opt_state['t']\n",
    "                \n",
    "                # Map layer IDs from saved state to current layers\n",
    "                # We need to create a mapping since object IDs will be different\n",
    "                dense_layers = [layer for layer in self.layers if isinstance(layer, Dense)]\n",
    "                saved_layer_ids = list(opt_state['m_weights'].keys())\n",
    "                \n",
    "                # Create new moment estimates with current layer IDs\n",
    "                for i, layer in enumerate(dense_layers):\n",
    "                    if i < len(saved_layer_ids):\n",
    "                        saved_id = saved_layer_ids[i]\n",
    "                        current_id = id(layer)\n",
    "                        \n",
    "                        self.optimizer.m_weights[current_id] = opt_state['m_weights'][saved_id].copy()\n",
    "                        self.optimizer.v_weights[current_id] = opt_state['v_weights'][saved_id].copy()\n",
    "                        self.optimizer.m_biases[current_id] = opt_state['m_biases'][saved_id].copy()\n",
    "                        self.optimizer.v_biases[current_id] = opt_state['v_biases'][saved_id].copy()\n",
    "    \n",
    "    def clone_to(self, target_network):\n",
    "        \"\"\"\n",
    "        Clone this network's state to another network\n",
    "        \n",
    "        Args:\n",
    "            target_network (NeuralNetwork): Target network to clone to\n",
    "        \"\"\"\n",
    "        state = self.get_state()\n",
    "        target_network.load_state(state)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a85945b9-7c5f-42cf-82af-3dcf1c7d4fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    def __init__(self, maxlen):\n",
    "        self.memory = deque([], maxlen=maxlen)\n",
    "\n",
    "    def append(self, transition):\n",
    "        self.memory.append(transition)\n",
    "\n",
    "    def sample(self, sample_size):\n",
    "        return random.sample(self.memory, sample_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fcc444cd-f697-4c48-a639-b8ec80b31dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleDQN():\n",
    "    buffer_size = 100_000\n",
    "    learning_rate = 1\n",
    "\n",
    "    def train(self, episodes):\n",
    "        env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "        num_states = env.observation_space.shape[0]\n",
    "        num_actions = env.action_space.n\n",
    "        terminated = False\n",
    "        truncated = False\n",
    "\n",
    "        cart_pos = np.linspace(env.observation_space.low[0], env.observation_space.high[0], 30)\n",
    "        cart_vel = np.linspace(env.observation_space.low[1], env.observation_space.high[1], 30)\n",
    "        pole_angle = np.linspace(env.observation_space.low[2], env.observation_space.high[2], 30)\n",
    "        pole_vel = np.linspace(env.observation_space.low[3], env.observation_space.high[3], 30)\n",
    "\n",
    "        epsilon = 1\n",
    "\n",
    "        self.memory = ReplayMemory(self.buffer_size)\n",
    "        policy_dqn = NeuralNetwork(num_states, 10, num_actions)\n",
    "        target_dqn = NeuralNetwork(num_states, 10, num_actions)\n",
    "\n",
    "        policy_dqn.clone_to(target_dqn)\n",
    "\n",
    "        self.optimizer = Adam(learning_rate=self.learning_rate)\n",
    "        \n",
    "        while not terminated and not truncated:\n",
    "            new_state, reward, terminated, truncated, _ = env.step(env.action_space.sample())\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "677a1bd6-05e7-4335-b7b6-28a6a56cfe19",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NeuralNetwork' object has no attribute 'optimizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m acrobot = CartPoleDQN()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43macrobot\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mCartPoleDQN.train\u001b[39m\u001b[34m(self, episodes)\u001b[39m\n\u001b[32m     21\u001b[39m policy_dqn = NeuralNetwork(num_states, \u001b[32m10\u001b[39m, num_actions)\n\u001b[32m     22\u001b[39m target_dqn = NeuralNetwork(num_states, \u001b[32m10\u001b[39m, num_actions)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mpolicy_dqn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclone_to\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_dqn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer = Adam(learning_rate=\u001b[38;5;28mself\u001b[39m.learning_rate)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m truncated:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 140\u001b[39m, in \u001b[36mNeuralNetwork.clone_to\u001b[39m\u001b[34m(self, target_network)\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclone_to\u001b[39m(\u001b[38;5;28mself\u001b[39m, target_network):\n\u001b[32m    134\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[33;03m    Clone this network's state to another network\u001b[39;00m\n\u001b[32m    136\u001b[39m \n\u001b[32m    137\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    138\u001b[39m \u001b[33;03m        target_network (NeuralNetwork): Target network to clone to\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     state = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m     target_network.load_state(state)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mNeuralNetwork.get_state\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_state\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m     21\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[33;03m    Get the current state of the network including weights, biases, and optimizer state\u001b[39;00m\n\u001b[32m     23\u001b[39m \n\u001b[32m     24\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33;03m        dict: Dictionary containing all network parameters and optimizer state\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     27\u001b[39m     state = {\n\u001b[32m     28\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mlayers\u001b[39m\u001b[33m'\u001b[39m: [],\n\u001b[32m     29\u001b[39m         \u001b[33m'\u001b[39m\u001b[33moptimizer_state\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m         \u001b[33m'\u001b[39m\u001b[33moptimizer_type\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer\u001b[49m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m,\n\u001b[32m     31\u001b[39m         \u001b[33m'\u001b[39m\u001b[33moptimizer_params\u001b[39m\u001b[33m'\u001b[39m: {}\n\u001b[32m     32\u001b[39m     }\n\u001b[32m     34\u001b[39m     \u001b[38;5;66;03m# Save layer parameters\u001b[39;00m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layers):\n",
      "\u001b[31mAttributeError\u001b[39m: 'NeuralNetwork' object has no attribute 'optimizer'"
     ]
    }
   ],
   "source": [
    "acrobot = CartPoleDQN()\n",
    "acrobot.train(episodes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788db49b-0435-4e64-b0f9-e8e15ea7dffd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
