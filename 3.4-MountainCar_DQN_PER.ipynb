{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5076be59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98d777ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_states, h1_nodes, out_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(in_states, h1_nodes)\n",
    "        self.out = nn.Linear(h1_nodes, out_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.out(F.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0a1de87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SumTree():\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.tree = np.zeros(2 * capacity - 1, dtype=np.float32)\n",
    "        self.data_pointer = 0\n",
    "    \n",
    "    def update(self, idx, priority):\n",
    "        tree_index = idx + self.capacity - 1\n",
    "        change = priority - self.tree[tree_index]\n",
    "        self.tree[tree_index] = priority\n",
    "        while tree_index != 0:\n",
    "            tree_index = (tree_index - 1) // 2\n",
    "            self.tree[tree_index] += change\n",
    "        \n",
    "    def get_leaf(self, idx):\n",
    "        parent_index = 0\n",
    "        while True:\n",
    "            left_child = 2 * parent_index + 1\n",
    "            right_child = left_child + 1\n",
    "\n",
    "            if left_child >= len(self.tree):\n",
    "                leaf_index = parent_index\n",
    "                break\n",
    "            if idx <= self.tree[left_child]:\n",
    "                parent_index = left_child\n",
    "            else:\n",
    "                idx -= self.tree[left_child]\n",
    "                parent_index = right_child\n",
    "        data_index = leaf_index - self.capacity + 1\n",
    "        return data_index, self.tree[leaf_index]\n",
    "    \n",
    "    def total_sum(self):\n",
    "        return self.tree[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "602e7945",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayMemory():\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha  # alpha = 0.6 (controls how much prioritization is used) 0 = no prioritization, 1 = full prioritization\n",
    "        self.priorities = SumTree(capacity)\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "        self.max_priority = 1.0\n",
    "        \n",
    "    def store(self, state, action, reward, next_state, done, priority):\n",
    "        experience = (state, action, reward, next_state, done)\n",
    "        \n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(experience)\n",
    "        else:\n",
    "            self.buffer[self.position] = experience\n",
    "        \n",
    "        self.priorities.update(self.position, priority ** self.alpha)\n",
    "        self.max_priority = max(self.max_priority, priority)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "\n",
    "    def update_priority(self, index, priority):\n",
    "        self.priorities.update(index, priority ** self.alpha)\n",
    "        self.max_priority = max(self.max_priority, priority)\n",
    "    \n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        indices = []\n",
    "        priorities = []\n",
    "        experiences = []\n",
    "\n",
    "        # sample based on priority\n",
    "        total_priority = self.priorities.total_sum()\n",
    "        segment_size = total_priority / batch_size\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            a = segment_size * i\n",
    "            b = segment_size * (i + 1)\n",
    "            value = random.uniform(a, b)\n",
    "\n",
    "            index, priority = self.priorities.get_leaf(value)\n",
    "            indices.append(index)\n",
    "            priorities.append(priority)\n",
    "            experiences.append(self.buffer[index])\n",
    "\n",
    "        # calculate importance sampling weights\n",
    "        weights = []\n",
    "        min_prob = min(priorities) / total_priority\n",
    "        max_weight = (min_prob * len(self.buffer)) ** (-beta)\n",
    "\n",
    "        for priority in priorities:\n",
    "            prob = priority / total_priority\n",
    "            weight = (prob * len(self.buffer)) ** (-beta)\n",
    "            weights.append(weight / max_weight)\n",
    "        \n",
    "        return experiences, indices, weights\n",
    "\n",
    "    def get_max_priority(self):\n",
    "        return self.max_priority\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ed468c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MountainCarDQL():\n",
    "    learning_rate_a = 0.01\n",
    "    discount_factor_g = 0.9  ## gamma\n",
    "    network_sync_rate = 50000\n",
    "    replay_memory_size = 100000\n",
    "    mini_batch_size = 32\n",
    "    num_divisions = 20\n",
    "\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = None\n",
    "    memory = None\n",
    "\n",
    "    def train(self, episodes):\n",
    "        env = gym.make('MountainCar-v0')\n",
    "        num_states = env.observation_space.shape[0]\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        # Divide position and velocity into segments\n",
    "        self.pos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)    # Between -1.2 and 0.6\n",
    "        self.vel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)    # Between -0.07 and 0.07\n",
    "\n",
    "        epsilon = 1\n",
    "        self.memory = PrioritizedReplayMemory(self.replay_memory_size)\n",
    "        policy_dqn = DQN(num_states, 10, num_actions)\n",
    "        target_dqn = DQN(num_states, 10, num_actions)\n",
    "\n",
    "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
    "\n",
    "        rewards_per_episode = []\n",
    "\n",
    "        epsilon_history = []\n",
    "\n",
    "        step_count=0\n",
    "        goal_reached=False\n",
    "        best_rewards=-200\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()[0]  # Initialize to state 0\n",
    "            terminated = False      # True when agent falls in hole or reached goal\n",
    "\n",
    "            rewards = 0\n",
    "\n",
    "            while(not terminated and rewards>-1000):\n",
    "                if random.random() < epsilon:\n",
    "                    # select random action\n",
    "                    action = env.action_space.sample() # actions: 0=left,1=idle,2=right\n",
    "                else:\n",
    "                    # select best action            \n",
    "                    with torch.no_grad():\n",
    "                        action = policy_dqn(self.state_to_dqn_input(state)).argmax().item()\n",
    "                new_state,reward,terminated,truncated,_ = env.step(action)\n",
    "\n",
    "                rewards += reward\n",
    "                max_priority = self.memory.get_max_priority()\n",
    "                if max_priority == 0:  # if memory is empty, set max_priority to 1.0\n",
    "                    max_priority = 1.0\n",
    "                self.memory.store(state, action, reward, new_state, terminated, max_priority)\n",
    "                state = new_state\n",
    "                step_count+=1\n",
    "\n",
    "            rewards_per_episode.append(rewards)\n",
    "            if(terminated):\n",
    "                goal_reached = True\n",
    "\n",
    "            # Graph training progress\n",
    "            if(i!=0 and i%1000==0):\n",
    "                print(f'Episode {i} Epsilon {epsilon}')\n",
    "                self.plot_progress(rewards_per_episode, epsilon_history)\n",
    "\n",
    "            if rewards>best_rewards:\n",
    "                best_rewards = rewards\n",
    "                print(f'Best rewards so far: {best_rewards}')\n",
    "                # Save policy\n",
    "                torch.save(policy_dqn.state_dict(), f\"mountaincar_dql_{i}.pt\")\n",
    "\n",
    "            if len(self.memory)>self.mini_batch_size and goal_reached:\n",
    "                mini_batch, indices, weights = self.memory.sample(self.mini_batch_size)\n",
    "\n",
    "                self.optimize(mini_batch, indices, weights, policy_dqn, target_dqn)        \n",
    "\n",
    "                # Decay epsilon\n",
    "                epsilon = max(epsilon - 1/episodes, 0)\n",
    "                epsilon_history.append(epsilon)\n",
    "\n",
    "                # Copy policy network to target network after a certain number of steps\n",
    "                if step_count > self.network_sync_rate:\n",
    "                    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
    "                    step_count=0    \n",
    "\n",
    "    def state_to_dqn_input(self, state)->torch.Tensor:\n",
    "        state_p = np.digitize(state[0], self.pos_space)\n",
    "        state_v = np.digitize(state[1], self.vel_space)\n",
    "        \n",
    "        return torch.FloatTensor([state_p, state_v])\n",
    "         \n",
    "    def optimize(self, mini_batch, indices, weights, policy_dqn, target_dqn):\n",
    "\n",
    "        current_q_list = []\n",
    "        target_q_list = []\n",
    "        td_errors = []\n",
    "\n",
    "        for state, action, reward, new_state, terminated in mini_batch:\n",
    "            if terminated: \n",
    "                target = torch.FloatTensor([reward])\n",
    "            else:\n",
    "                # Calculate target q value \n",
    "                with torch.no_grad():\n",
    "                    target = torch.FloatTensor(\n",
    "                        reward + self.discount_factor_g * target_dqn(self.state_to_dqn_input(new_state)).max()\n",
    "                    )\n",
    "\n",
    "            # Get the current set of Q values\n",
    "            current_q = policy_dqn(self.state_to_dqn_input(state))\n",
    "            current_q_list.append(current_q)\n",
    "\n",
    "            # Get the target set of Q values\n",
    "            target_q = target_dqn(self.state_to_dqn_input(state)) \n",
    "            # Adjust the specific action to the target that was just calculated\n",
    "            target_q[action] = target\n",
    "            target_q_list.append(target_q)\n",
    "\n",
    "            # Calculate TD error\n",
    "            td_error = target - current_q[action]\n",
    "            td_errors.append(td_error.item())\n",
    "\n",
    "\n",
    "        # Compute loss for the whole minibatch\n",
    "        loss = self.loss_fn(torch.stack(current_q_list), torch.stack(target_q_list))\n",
    "\n",
    "        # Apply importance sampling weights\n",
    "        weights = torch.FloatTensor(weights)\n",
    "        loss = (loss * weights).mean()\n",
    "        \n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update priorities in the replay memory\n",
    "        for idx, td_error in zip(indices, td_errors):\n",
    "            priority = abs(td_error) + 1e-5\n",
    "            self.memory.update_priority(idx, priority)\n",
    "\n",
    "    def plot_progress(self, rewards_per_episode, epsilon_history):\n",
    "        # Create new graph \n",
    "        plt.figure(1)\n",
    "\n",
    "        # Plot average rewards (Y-axis) vs episodes (X-axis)\n",
    "        # rewards_curve = np.zeros(len(rewards_per_episode))\n",
    "        # for x in range(len(rewards_per_episode)):\n",
    "            # rewards_curve[x] = np.min(rewards_per_episode[max(0, x-10):(x+1)])\n",
    "        plt.subplot(121) # plot on a 1 row x 2 col grid, at cell 1\n",
    "        # plt.plot(sum_rewards)\n",
    "        plt.plot(rewards_per_episode)\n",
    "        \n",
    "        # Plot epsilon decay (Y-axis) vs episodes (X-axis)\n",
    "        plt.subplot(122) # plot on a 1 row x 2 col grid, at cell 2\n",
    "        plt.plot(epsilon_history)\n",
    "        plt.savefig('mountaincar_dql.png')\n",
    "\n",
    "    # Run the environment with the learned policy\n",
    "    def test(self, episodes, model_filepath):\n",
    "        # Create FrozenLake instance\n",
    "        env = gym.make('MountainCar-v0', render_mode='human')\n",
    "        num_states = env.observation_space.shape[0]\n",
    "        num_actions = env.action_space.n\n",
    "\n",
    "        self.pos_space = np.linspace(env.observation_space.low[0], env.observation_space.high[0], self.num_divisions)    # Between -1.2 and 0.6\n",
    "        self.vel_space = np.linspace(env.observation_space.low[1], env.observation_space.high[1], self.num_divisions)    # Between -0.07 and 0.07\n",
    "\n",
    "        # Load learned policy\n",
    "        policy_dqn = DQN(in_states=num_states, h1_nodes=10, out_actions=num_actions) \n",
    "        policy_dqn.load_state_dict(torch.load(model_filepath))\n",
    "        policy_dqn.eval()    # switch model to evaluation mode\n",
    "\n",
    "        for i in range(episodes):\n",
    "            state = env.reset()[0]  # Initialize to state 0\n",
    "            terminated = False      # True when agent falls in hole or reached goal\n",
    "            truncated = False       # True when agent takes more than 200 actions            \n",
    "\n",
    "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
    "            while(not terminated and not truncated):  \n",
    "                # Select best action   \n",
    "                with torch.no_grad():\n",
    "                    action = policy_dqn(self.state_to_dqn_input(state)).argmax().item()\n",
    "\n",
    "                # Execute action\n",
    "                state,reward,terminated,truncated,_ = env.step(action)\n",
    "\n",
    "        env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4b7185",
   "metadata": {},
   "outputs": [],
   "source": [
    "mountaincar = MountainCarDQL()\n",
    "#mountaincar.train(20000)\n",
    "mountaincar.test(3, 'mountaincar_dql_19284.pt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23374496",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
